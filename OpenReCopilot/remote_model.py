import openai # pip install openai
import time
import asyncio
import traceback # ç”¨äºæ‰“å°å¼‚å¸¸å †æ ˆ

# å‡è®¾è¿™äº›è‡ªå®šä¹‰æ¨¡å—å­˜åœ¨äºé¡¹ç›®ä¸­
from config import settings_manager, PROMPT_TEMPLATE # PROMPT_TEMPLATE å¯èƒ½æ˜¯å­—å…¸
from task_guides import TASK_GUIDES, TASK_OUTPUT_FORMATS, get_mock_response

# PyArmor ç›¸å…³çš„ __assert_armored__, __pyarmor_enter_XXXX__, __pyarmor_exit_XXXX__ è°ƒç”¨å·²çœç•¥

class OpenAIModel:
    def __init__(self):
        # PyArmor ä¿æŠ¤ä»£ç å·²çœç•¥
        super().__init__() # è™½ç„¶å­—èŠ‚ç ä¸­æ²¡æœ‰æ˜ç¡®çš„çˆ¶ç±»ï¼Œä½† super() è°ƒç”¨é€šå¸¸å­˜åœ¨
        self._current_completion = None # ç”¨äºå­˜å‚¨æ´»åŠ¨çš„æµå¼ API è°ƒç”¨å¯¹è±¡
        self._cancelled = False         # å–æ¶ˆæ ‡å¿—

    def cancel(self):
        """å–æ¶ˆå½“å‰æ­£åœ¨è¿›è¡Œçš„æ¨¡å‹è°ƒç”¨ã€‚"""
        # PyArmor ä¿æŠ¤ä»£ç å·²çœç•¥
        self._cancelled = True

    async def call_model(self, prompt: str, task_tag: str, timeout: int = 600):
        """
        å¼‚æ­¥è°ƒç”¨ OpenAI æ¨¡å‹ã€‚

        Args:
            prompt: ç”¨æˆ·æä¾›çš„æ ¸å¿ƒæç¤ºå†…å®¹ã€‚
            task_tag: ä»»åŠ¡çš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œç”¨äºé€‰æ‹©ä»»åŠ¡æŒ‡å—å’Œè¾“å‡ºæ ¼å¼ã€‚
            timeout: API è°ƒç”¨çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ã€‚

        Returns:
            ä¸€ä¸ªå…ƒç»„ (model_response_text, original_prompt_for_feedback)ã€‚
            å¦‚æœå‘ç”Ÿé”™è¯¯æˆ–å–æ¶ˆï¼Œmodel_response_text ä¼šåŒ…å«é”™è¯¯æˆ–å–æ¶ˆä¿¡æ¯ã€‚
        """
        # PyArmor ä¿æŠ¤ä»£ç å·²çœç•¥
        self._cancelled = False # é‡ç½®å–æ¶ˆæ ‡å¿—
        
        # 1. è·å–å¹¶æ ¼å¼åŒ–æç¤ºæ¨¡æ¿
        template_name = settings_manager.settings.get('prompt_template', 'default_template_name') # ä»é…ç½®è·å–æ¨¡æ¿åç§°
        # å‡è®¾ PROMPT_TEMPLATE æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œä¾‹å¦‚:
        # PROMPT_TEMPLATE = {
        #     "default_template_name": "Format: {format}\nGuide: {guide}\nInput: {input}",
        #     "default_template_name_wo_guide": "Format: {format}\nInput: {input}"
        # }
        
        current_template_str = PROMPT_TEMPLATE.get(template_name, "{input}") # è·å–æ¨¡æ¿å­—ç¬¦ä¸²ï¼Œæä¾›é»˜è®¤å€¼

        if template_name.endswith("_wo_guide"): # å¦‚æœæ¨¡æ¿åç§°æŒ‡ç¤ºä¸ä½¿ç”¨æŒ‡å—
            formatted_prompt = current_template_str.replace("{format}", TASK_OUTPUT_FORMATS.get(task_tag, "")) \
                                                 .replace("{input}", prompt)
        else:
            formatted_prompt = current_template_str.replace("{format}", TASK_OUTPUT_FORMATS.get(task_tag, "")) \
                                                 .replace("{guide}", TASK_GUIDES.get(task_tag, "")) \
                                                 .replace("{input}", prompt)
        
        # ä¸ºåé¦ˆå‡†å¤‡çš„åŸå§‹æç¤ºï¼ˆå¯èƒ½å°±æ˜¯æ ¼å¼åŒ–åçš„æç¤ºï¼‰
        prompt_for_feedback = formatted_prompt 

        # 2. æ‰“å°è°ƒè¯•ä¿¡æ¯ (å¦‚æœå¯ç”¨äº†è°ƒè¯•æ¨¡å¼)
        if settings_manager.settings.get('debug_mode', False):
            # å­—èŠ‚ç ä¸­å¯¹ prompt è¿›è¡Œäº† .split('\n') å’Œ .join('\n[DEBUGğŸ›] ') æ“ä½œ
            debug_prompt_lines = [f"\n[DEBUGğŸ›] {line}" for line in formatted_prompt.split('\n')]
            print("".join(debug_prompt_lines))

            model_name_setting = settings_manager.settings.get('model_name', 'unknown_model')
            print(f"[ğŸ”—] OpenAIModel.call_model: model_name={model_name_setting}, timeout={timeout}s")
            print(f"[ğŸ”—] OpenAIModel.call_model: send {len(formatted_prompt)} chars prompt")

        # 3. åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        try:
            client_args = {}
            if settings_manager.settings.get('base_url'):
                client_args['base_url'] = settings_manager.settings['base_url']
            if settings_manager.settings.get('api_key'):
                client_args['api_key'] = settings_manager.settings['api_key']
            
            client = openai.OpenAI(**client_args)

            # 4. æ„å»ºæ¶ˆæ¯å¹¶å‘é€è¯·æ±‚
            messages = [{"role": "user", "content": formatted_prompt}]
            
            self._current_completion = await client.chat.completions.create(
                model=settings_manager.settings.get('model_name', 'gpt-3.5-turbo'), # ä»é…ç½®è·å–æ¨¡å‹åç§°
                temperature=0.6,
                stream=True,
                max_tokens=settings_manager.settings.get('max_output_tokens', 2048), # ä»é…ç½®è·å–
                messages=messages,
                timeout=float(timeout) # ç¡®ä¿æ˜¯æµ®ç‚¹æ•°
            )

            # 5. å¤„ç†æµå¼å“åº”
            reasoning_content_parts = []
            async for chunk in self._current_completion:
                if self._cancelled:
                    print("\n[!ğŸ’¥] Analysis cancelled by user (during streaming)")
                    await self._current_completion.close() # ç¡®ä¿å…³é—­æµ
                    self._current_completion = None
                    return "<Cancelled>Analysis cancelled by user", prompt_for_feedback
                
                # æ£€æŸ¥ chunk.choices[0].delta.content
                # å­—èŠ‚ç ä¸­æ£€æŸ¥äº† hasattr(chunk.choices[0].delta, 'reasoning_content')
                # å’Œ hasattr(chunk.choices[0].delta, 'content')
                # è¿™è¡¨æ˜æ¨¡å‹å¯èƒ½è¿”å›å¸¦æœ‰ 'reasoning_content' æˆ– 'content' çš„ delta
                
                chunk_content = None
                if chunk.choices and chunk.choices[0].delta:
                    delta = chunk.choices[0].delta
                    if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                        chunk_content = delta.reasoning_content
                    elif hasattr(delta, 'content') and delta.content:
                         chunk_content = delta.content
                
                if chunk_content:
                    print(chunk_content, end="") # å®æ—¶æ‰“å°ï¼Œä¸æ¢è¡Œ
                    reasoning_content_parts.append(chunk_content)
            
            print() # åœ¨æµç»“æŸåæ¢è¡Œ
            self._current_completion = None
            final_response_text = "".join(reasoning_content_parts)
            return final_response_text, prompt_for_feedback

        except openai.Timeout as e: # æ›´å…·ä½“çš„ OpenAI è¶…æ—¶
            print(f"[!ğŸ’¥] Error: OpenAI API request timed out: {e}")
            self._current_completion = None # æ¸…ç†
            return f"<RequestException>Request timed out: {e}", prompt_for_feedback
        except Exception as e:
            print(f"[!ğŸ’¥] Error in OpenAIModel.call_model: {e}")
            traceback.print_exc()
            self._current_completion = None # æ¸…ç†
            return f"<RequestException>{str(e)}", prompt_for_feedback

    async def call_model_mock(self, prompt: str, task_tag: str, timeout: int = 600):
        """
        å¼‚æ­¥æ¨¡æ‹Ÿè°ƒç”¨AIæ¨¡å‹ï¼Œç”¨äºè°ƒè¯•ã€‚
        """
        # PyArmor ä¿æŠ¤ä»£ç å·²çœç•¥
        self._cancelled = False
        
        template_name = settings_manager.settings.get('prompt_template', 'default_template_name')
        current_template_str = PROMPT_TEMPLATE.get(template_name, "{input}")

        if template_name.endswith("_wo_guide"):
            formatted_prompt = current_template_str.replace("{format}", TASK_OUTPUT_FORMATS.get(task_tag, "")) \
                                                 .replace("{input}", prompt)
        else:
            formatted_prompt = current_template_str.replace("{format}", TASK_OUTPUT_FORMATS.get(task_tag, "")) \
                                                 .replace("{guide}", TASK_GUIDES.get(task_tag, "")) \
                                                 .replace("{input}", prompt)
        
        prompt_for_feedback = formatted_prompt

        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        if settings_manager.settings.get('debug_mode', False):
            debug_prompt_lines = [f"\n[DEBUGğŸ›] {line}" for line in formatted_prompt.split('\n')]
            print("".join(debug_prompt_lines))
            
            base_url_setting = settings_manager.settings.get('base_url', 'N/A')
            api_key_setting = settings_manager.settings.get('api_key', 'N/A')[:5] + "..." # ä»…æ˜¾ç¤ºéƒ¨åˆ†APIå¯†é’¥
            model_name_setting = settings_manager.settings.get('model_name', 'mock_model')
            print(f"[DEBUGğŸ›] OpenAIModel.call_model_mock: base_url={base_url_setting}, api_key={api_key_setting}, model_name={model_name_setting}, timeout={timeout}s")
            print(f"[DEBUGğŸ›] OpenAIModel.call_model_mock: recv {len(formatted_prompt)} chars prompt")

        mock_response_full = get_mock_response(task_tag)
        
        print(f"[DEBUGğŸ›] Mock response for {task_tag}:")
        response_parts = []
        for line in mock_response_full.split('\n'): # æ¨¡æ‹Ÿæµå¼è¾“å‡º
            if self._cancelled:
                print("\n[!ğŸ’¥] Analysis cancelled by user (during mock streaming)")
                return "<Cancelled>Analysis cancelled by user", prompt_for_feedback
            
            print(f"[DEBUGğŸ›] {line}") # é€è¡Œæ‰“å°æ¨¡æ‹Ÿå“åº”
            response_parts.append(line)
            await asyncio.sleep(0.1) # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ

        return "\n".join(response_parts), prompt_for_feedback